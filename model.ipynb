{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b1c2125e",
   "metadata": {},
   "source": [
    "### Klasyfikacja ręcznie rysowanych i zdigitalizowanych obrazków\n",
    "\n",
    "Celem tego zadania jest zbudowanie i ocena modelu uczenia maszynowego do klasyfikacji prostych, ręcznie rysowanych obrazów. Zadanie polega na przetworzeniu i analizie zbioru danych zawierającego 10 różnych klas obiektów, z których każda reprezentowana jest przez trzy typy obrazów: rysowane i fotografowane, rysowane i skanowane oraz tworzone za pomocą pieczątek.\n",
    "\n",
    "Projekt zostanie zrealizowany w czterech głównych etapach:\n",
    "1.  **Eksploracja danych:** zapoznanie się ze zbiorem danych, jego strukturą, rozkładem klas i potencjalnymi problemami\n",
    "2.  **Preprocessing:** przygotowanie obrazów do treningu modelu, w tym normalizacja danych\n",
    "3.  **Modelowanie:** zbudowanie, wytrenowanie i porównanie trzech różnych architektur sieci konwolucyjnych (CNN)\n",
    "4.  **Ewaluacja:** ocena wydajności modeli przy użyciu odpowiednich metryk i analiza wyników"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a81e9d7",
   "metadata": {},
   "source": [
    "##### 1. Eksploracja danych\n",
    "Na tym etapie ładujemy zbiór danych i przeprowadzamy jego wstępną analizę. Naszym celem jest zrozumienie jego podstawowych cech, takich jak liczba klas, liczba obrazków w każdej klasie oraz wizualna prezentacja próbek. Pozwoli nam to zidentyfikować potencjalne problemy, takie jak niezbalansowanie klas, które mogłyby wpłynąć na proces trenowania modelu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72ab218e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torchvision\n",
    "import torch\n",
    "import random\n",
    "import collections\n",
    "from tqdm import tqdm\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import classification_report, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d31d6fd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ładowanie datasetu\n",
    "full_ds = torchvision.datasets.ImageFolder(root=\"data\")\n",
    "class_names = full_ds.classes\n",
    "num_classes = len(class_names)\n",
    "\n",
    "print(f\"Liczba obrazków: {len(full_ds)}\")\n",
    "print(\"Liczba klas: \", num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96421a66",
   "metadata": {},
   "source": [
    "**Podział na zbiór treningowy i testowy**\n",
    "\n",
    "Kluczowym krokiem jest podział danych na zbiór treningowy i testowy. Zbiór testowy zostanie całkowicie wyłączony z dalszej analizy i procesu trenowania. Użyjemy podziału 80/20, gdzie 80% danych posłuży do trenowania i walidacji, a pozostałe 20% do testowania."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fca6cc19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dzielenie datasetu na train i test\n",
    "train_val_size = int(0.8 * len(full_ds))\n",
    "test_size = len(full_ds) - train_val_size\n",
    "train_val_ds, test_ds = torch.utils.data.random_split(full_ds, [train_val_size, test_size])\n",
    "train_val_targets = np.array(full_ds.targets)[train_val_ds.indices]\n",
    "\n",
    "\n",
    "# badanie zrównoważenia klas w zbiorze treningowym\n",
    "class_counter = collections.Counter(train_val_targets)\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.bar(class_counter.keys(), class_counter.values())\n",
    "plt.xticks(range(num_classes), class_names, rotation=45, ha=\"right\")\n",
    "plt.xlabel(\"Klasy\")\n",
    "plt.ylabel(\"Liczba obrazków\")\n",
    "plt.title(\"Liczebność klas w zbiorze treningowym i walidacyjnym\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"figures/liczebnosc-klas.png\")\n",
    "plt.show()\n",
    "\n",
    "# wizualizacja zbioru treningowego\n",
    "fig, axes = plt.subplots(2, 5, figsize=(15, 7))\n",
    "fig.suptitle(\"Przykładowe obrazki ze zbioru treningowego\", fontsize=16)\n",
    "for i, class_name in enumerate(class_names):\n",
    "    class_idx = full_ds.class_to_idx[class_name]\n",
    "    target_ixs = np.where(train_val_targets == class_idx)[0]\n",
    "    \n",
    "    ix = random.choice(target_ixs)\n",
    "    img, label = train_val_ds[ix]\n",
    "    \n",
    "    ax = axes.flat[i]\n",
    "    ax.imshow(img)\n",
    "    ax.set_title(f\"Klasa: {class_name}\")\n",
    "    ax.axis('off')\n",
    "\n",
    "# zapisywanie wizualizacji\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"figures/ds-wizualizacja.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3ad4b16",
   "metadata": {},
   "source": [
    "**Wnioski z przeprowadzonej eksploracji danych** \n",
    "\n",
    "Jak widać na powyższym wykresie, zbiór treningowy jest stosunkowo dobrze zbalansowany. Liczba obrazków w poszczególnych klasach jest do siebie zbliżona, co jest korzystne dla procesu uczenia i zmniejsza ryzyko, że model będzie faworyzował klasy liczniejsze."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba0b47e6",
   "metadata": {},
   "source": [
    "#### 2. Preprocessing \n",
    "\n",
    "W tym etapie surowe obrazki (w formacie PIL) zostaną przekształcone do postaci tensorów. Ponadto, przeprowadzimy normalizację, która polega na przeskalowaniu wartości pikseli tak, aby miały średnią równą 0 i odchylenie standardowe równe 1. Obliczymy średnią i odchylenie standardowe na podstawie zbioru treningowego, aby uniknąć wycieku informacji ze zbioru testowego."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87cebaae",
   "metadata": {},
   "outputs": [],
   "source": [
    "calc_transform = torchvision.transforms.ToTensor()\n",
    "\n",
    "train_val_ds_for_calc = torchvision.datasets.ImageFolder(root=\"data\", transform=calc_transform)\n",
    "train_val_ds_for_calc = torch.utils.data.Subset(train_val_ds_for_calc, train_val_ds.indices)\n",
    "\n",
    "calc_loader = torch.utils.data.DataLoader(train_val_ds_for_calc, batch_size=64, num_workers=2)\n",
    "\n",
    "mean = 0\n",
    "std = 0\n",
    "n_samples = 0\n",
    "\n",
    "for images_batch, labels_batch in calc_loader:\n",
    "    batch_samples = images_batch.size(0)\n",
    "    images_flat = images_batch.view(batch_samples, images_batch.size(1), -1)\n",
    "    mean += images_flat.mean(2).sum(0)\n",
    "    std += images_flat.std(2).sum(0)\n",
    "    n_samples += batch_samples\n",
    "\n",
    "mean /= n_samples\n",
    "std /= n_samples\n",
    "\n",
    "print(f\"Obliczone wartości dla zbioru treningowego:\")\n",
    "print(f\"Średnia: {mean}\")\n",
    "print(f\"Odchylenie standardowe: {std}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f521bc8c",
   "metadata": {},
   "source": [
    "Zdefiniujemy dwie różne ścieżki transformacji obrazów:\n",
    "\n",
    "1.  **Transformacja standardowa (`transform_norm`):** Używana dla naszych własnych, prostych modeli CNN. Obejmuje konwersję do tensora i normalizację z wykorzystaniem obliczonych wcześniej statystyk.\n",
    "\n",
    "2.  **Transformacja dla modelu pretrained (`transform_pretrained`):** Używana dla modelu ResNet18. Oprócz konwersji do tensora, obrazy są skalowane do rozmiaru 224x224, a normalizacja odbywa się z użyciem standardowych wartości dla zbioru ImageNet, na którym model był pierwotnie trenowany.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b9d7772",
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalizacja zbioru dla modelu 1 i 2\n",
    "transform_norm = torchvision.transforms.Compose([torchvision.transforms.ToTensor(), torchvision.transforms.Normalize(mean=mean, std=std)])\n",
    "\n",
    "# normalizacja zbioru dla modelu 3 (ResNet18)\n",
    "transform_pretrained = torchvision.transforms.Compose([\n",
    "    torchvision.transforms.Resize(256),\n",
    "    torchvision.transforms.CenterCrop(224),\n",
    "    torchvision.transforms.ToTensor(),\n",
    "    torchvision.transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# tworzenie zbiorów dla modeli 1 i 2\n",
    "full_ds_norm = torchvision.datasets.ImageFolder(root=\"data\", transform=transform_norm)\n",
    "train_val_ds_norm = torch.utils.data.Subset(full_ds_norm, train_val_ds.indices)\n",
    "test_ds_norm = torch.utils.data.Subset(full_ds_norm, test_ds.indices)\n",
    "\n",
    "# tworzenie zbiorów dla modelu 3 (ResNet18)\n",
    "full_ds_pretrained = torchvision.datasets.ImageFolder(root=\"data\", transform=transform_pretrained)\n",
    "train_val_ds_pretrained = torch.utils.data.Subset(full_ds_pretrained, train_val_ds.indices)\n",
    "test_ds_pretrained = torch.utils.data.Subset(full_ds_pretrained, test_ds.indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb087fad",
   "metadata": {},
   "source": [
    "#### 3. Modele\n",
    "\n",
    "W tej części zdefiniujemy, wytrenujemy i porównamy trzy różne architektury sieci konwolucyjnych, ponieważ są one standardem w problemach związanych z przetwarzaniem i klasyfikacją obrazów. Zaczniemy od prostego modelu CNN, następnie wprowadzimy ulepszenia w postaci batch normalization i dropoutu, a na końcu wykorzystamy architekturę ResNet18. Jako funkcję straty wybieramy `CrossEntropyLoss`, a jako optymalizator znany i lubiany - optymalizator `Adam`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91b134c1",
   "metadata": {},
   "source": [
    "- **Model 1 (CNN1)**: Prosta sieć z trzema warstwami konwolucyjnymi, każda z nich jest połączona z warstwą max-pooling, a następnie z warstwą w pełni połączoną"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a83f9915",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model 1\n",
    "class CNN1(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, input_size, num_classes):\n",
    "        super(CNN1, self).__init__()\n",
    "\n",
    "        self.conv_block1 = torch.nn.Sequential(\n",
    "        torch.nn.Conv2d(in_channels=input_size, out_channels=16, kernel_size=3, padding=1),\n",
    "        torch.nn.ReLU(),\n",
    "        torch.nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        )\n",
    "\n",
    "        self.conv_block2 = torch.nn.Sequential(\n",
    "        torch.nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3, padding=1),\n",
    "        torch.nn.ReLU(),\n",
    "        torch.nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        )\n",
    "        \n",
    "\n",
    "        self.conv_block3 = torch.nn.Sequential(\n",
    "        torch.nn.Conv2d(in_channels=32, out_channels=40, kernel_size=3, padding=1),\n",
    "        torch.nn.ReLU(),\n",
    "        torch.nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        )\n",
    "        \n",
    "        self.fc1 = torch.nn.Linear(40 * 16 * 20, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        x = self.conv_block1(x)\n",
    "        x = self.conv_block2(x)\n",
    "        x = self.conv_block3(x)\n",
    "\n",
    "        x = x.reshape(x.shape[0], -1)\n",
    "        x = self.fc1(x)\n",
    "\n",
    "        return x\n",
    "    \n",
    "model1 = CNN1(input_size=3,num_classes=len(class_names))\n",
    "\n",
    "# optimizer hyperparameters\n",
    "optimizer1 = torch.optim.Adam(model1.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c57d9cd7",
   "metadata": {},
   "source": [
    "* **Model 2 (CNN2):** To rozbudowana wersja pierwszego. Do każdego bloku konwolucyjnego dodajemy warstwę `BatchNorm2d`, która stabilizuje i przyspiesza proces uczenia. Przed warstwą klasyfikującą dodajemy również warstwę `Dropout` z prawdopodobieństwem 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "557654eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model 2\n",
    "class CNN2(torch.nn.Module):\n",
    "    def __init__(self, input_size, num_classes):\n",
    "        super(CNN2, self).__init__()\n",
    "\n",
    "        self.conv_block1 = torch.nn.Sequential(\n",
    "            torch.nn.Conv2d(in_channels=input_size, out_channels=16, kernel_size=3, padding=1),\n",
    "            torch.nn.BatchNorm2d(16),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        )\n",
    "\n",
    "        self.conv_block2 = torch.nn.Sequential(\n",
    "            torch.nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3, padding=1),\n",
    "            torch.nn.BatchNorm2d(32),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        )\n",
    "\n",
    "        self.conv_block3 = torch.nn.Sequential(\n",
    "            torch.nn.Conv2d(in_channels=32, out_channels=48, kernel_size=3, padding=1),\n",
    "            torch.nn.BatchNorm2d(48),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        )\n",
    "        \n",
    "        self.classifier = torch.nn.Sequential(\n",
    "            torch.nn.Flatten(),\n",
    "            torch.nn.Dropout(p=0.5),\n",
    "            torch.nn.Linear(48 * 16 * 20, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv_block1(x)\n",
    "        x = self.conv_block2(x)\n",
    "        x = self.conv_block3(x)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "model2 = CNN2(input_size=3, num_classes=len(class_names))\n",
    "\n",
    "# optimizer hyperparameters\n",
    "optimizer2 = torch.optim.Adam(model2.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c4da882",
   "metadata": {},
   "source": [
    "* **Model 3:** Trzecie podejście to transfer learning. Wykorzystujemy wytrenowany wcześniej model ResNet18. \"Zamrażamy\" wagi wszystkich warstw konwolucyjnych, aby zachować wiedzę zdobytą na ImageNet, a następnie zastępujemy oryginalną warstwę klasyfikującą nową, dostosowaną do naszego problemu (10 klas)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "379dcac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model3 = torchvision.models.resnet18(weights=torchvision.models.ResNet18_Weights.DEFAULT)\n",
    "\n",
    "for param in model3.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "num_ftrs = model3.fc.in_features\n",
    "model3.fc = torch.nn.Linear(num_ftrs, num_classes)\n",
    "\n",
    "optimizer3 = torch.optim.Adam(model3.fc.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0909a282",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 30\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "val_size = int(0.2 * len(train_val_ds))\n",
    "train_size = len(train_val_ds) - val_size\n",
    "\n",
    "# tworzenie loaderów dla modeli 1 i 2\n",
    "train_ds_norm, val_ds_norm = torch.utils.data.random_split(train_val_ds_norm, [train_size, val_size])\n",
    "train_loader_norm = torch.utils.data.DataLoader(train_ds_norm, batch_size=64, shuffle=True, num_workers=0)\n",
    "val_loader_norm = torch.utils.data.DataLoader(val_ds_norm, batch_size=64, shuffle=False, num_workers=0)\n",
    "test_loader_norm = torch.utils.data.DataLoader(test_ds_norm, batch_size=64, shuffle=False)\n",
    "\n",
    "# tworzenie loaderów dla modelu 3 (ResNet18)\n",
    "train_ds_pretrained, val_ds_pretrained = torch.utils.data.random_split(train_val_ds_pretrained, [train_size, val_size])\n",
    "train_loader_pretrained = torch.utils.data.DataLoader(train_ds_pretrained, batch_size=64, shuffle=True, num_workers=0)\n",
    "val_loader_pretrained = torch.utils.data.DataLoader(val_ds_pretrained, batch_size=64, shuffle=False, num_workers=0)\n",
    "test_loader_pretrained = torch.utils.data.DataLoader(test_ds_pretrained, batch_size=64, shuffle=False)\n",
    "\n",
    "\n",
    "print(f\"Wielkość zbioru treningowego: {len(train_ds_norm)}\")\n",
    "print(f\"Wielkość zbioru walidacyjnego: {len(val_ds_norm)}\")\n",
    "print(f\"Wielkość zbioru testowego: {len(test_ds_norm)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96c24ee8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# training loop model 1\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model1.train()\n",
    "    train_loss = 0.0\n",
    "    for images, labels in tqdm(train_loader_norm):\n",
    "        outputs = model1(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        optimizer1.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer1.step()\n",
    "        train_loss += loss.item()\n",
    "    model1.eval()\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in tqdm(val_loader_norm):\n",
    "            outputs = model1(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            val_loss += loss.item()\n",
    "\n",
    "    print(f\"Epoch [{epoch+1}/{epochs}], Train Loss: {train_loss/len(train_loader_norm):.4f}, Val Loss: {val_loss/len(val_loader_norm):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db5d81b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# training loop model 2\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model2.train()\n",
    "    train_loss = 0.0\n",
    "    for images, labels in tqdm(train_loader_norm):\n",
    "        outputs = model2(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        optimizer2.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer2.step()\n",
    "        train_loss += loss.item()\n",
    "    model2.eval()\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in tqdm(val_loader_norm):\n",
    "            outputs = model2(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            val_loss += loss.item()\n",
    "\n",
    "    print(f\"Epoch [{epoch+1}/{epochs}], Train Loss: {train_loss/len(train_loader_norm):.4f}, Val Loss: {val_loss/len(val_loader_norm):.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "543b1a59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# training loop model 3\n",
    "epochs = 30\n",
    "for epoch in range(epochs):\n",
    "    model3.train()\n",
    "    train_loss = 0.0\n",
    "    for images, labels in tqdm(train_loader_pretrained):\n",
    "        outputs = model3(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        optimizer3.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer3.step()\n",
    "        train_loss += loss.item()\n",
    "    model3.eval()\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in tqdm(val_loader_pretrained):\n",
    "            outputs = model3(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            val_loss += loss.item()\n",
    "\n",
    "    print(f\"Epoch [{epoch+1}/{epochs}], Train Loss: {train_loss/len(train_loader_pretrained):.4f}, Val Loss: {val_loss/len(val_loader_pretrained):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "669013e0",
   "metadata": {},
   "source": [
    "#### 4. Ewaluacja modeli\n",
    "\n",
    "Na tym etapie oceniamy ostateczną skuteczność modeli na zbiorze testowym. Celem jest weryfikacja, jak dobrze modele generalizują wiedzę na nowe dane.\n",
    "\n",
    "Wybrane metryki:\n",
    "\n",
    "*   **Raport klasyfikacji:** zamiast samej dokładności (Accuracy), analizujemy też inne metryki:\n",
    "    *   **Precision**\n",
    "    *   **Recall**\n",
    "    *   **F1-Score**\n",
    "\n",
    "*   **Confusion matrix:** pokazuje, które klasy są ze sobą najczęściej mylone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e805faa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, dataloader, model_name):\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, labels in dataloader:\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            all_preds.extend(predicted.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            \n",
    "    print(f\"--- Ewaluacja dla: {model_name} ---\")\n",
    "    \n",
    "    report = classification_report(all_labels, all_preds, target_names=class_names)\n",
    "    print(\"\\nRaport klasyfikacji:\")\n",
    "    print(report)\n",
    "    \n",
    "    cm = confusion_matrix(all_labels, all_preds)\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=class_names, yticklabels=class_names)\n",
    "    plt.xlabel('Przewidziana etykieta')\n",
    "    plt.ylabel('Prawdziwa etykieta')\n",
    "    plt.title(f'Confusion matrix dla {model_name}')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cfeb384",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ewaluacja modeli\n",
    "evaluate_model(model1, test_loader_norm, \"Model 1 (CNN1)\")\n",
    "evaluate_model(model2, test_loader_norm, \"Model 2 (CNN2)\")\n",
    "evaluate_model(model3, test_loader_pretrained, \"Model 3 (ResNet18)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64847121",
   "metadata": {},
   "source": [
    "**Wyniki pokazały wyraźną progresję:**\n",
    "* Prosty model CNN1 stanowił solidną bazę, ale miał trudności z niektórymi klasami\n",
    "* Ulepszony CNN2 z batch normalization i dropoutem wykazał znaczącą poprawę\n",
    "* Model oparty o transfer learning (ResNet18) okazał się zdecydowanie najskuteczniejszy, osiągając niemal bezbłędne wyniki i deklasując modele trenowane od zera\n",
    "\n",
    "**Analiza Błędów**\n",
    "* **Błędy modeli:** Słabością prostszych architektur było mylenie klas o podobnych cechach wizualnych (np. smiley, speech_bubble, spiral), co było widoczne na ich macierzach pomyłek. Model ResNet18 niemal całkowicie wyeliminował te problemy\n",
    "* **Wyciek danych:** Początkowo prawdopodobnie niepoprawne podzielenie i użycie datasetu, co mogło doprowadzić do wycieku danych\n",
    "\n",
    "**Miejsca do poprawy**\n",
    "\n",
    "Chociaż osiągnięto bardzo dobre wyniki, istnieją dalsze możliwości progresji:\n",
    "* **Augmentacja danych:** Zastosowanie technik takich jak losowe obroty czy zmiany jasności mogłoby jeszcze bardziej zwiększyć odporność modelu, nawet tego najlepszego\n",
    "* **Eksperymenty z hiperparametrami:** Można by przetestować inne optymalizatory, wartości współczynnika uczenia czy większą liczbę epok treningowych"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f741504",
   "metadata": {},
   "source": [
    "### Augmentacja danych\n",
    "Ponownie wytrenujemy **Model 2 (CNN2)** na zbiorze z augmentacją. Wybraliśmy ten model, ponieważ jego wyniki nie były idealne, co stwarza przestrzeń do widocznej poprawy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1b53ae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# transformacja z augmentacją dla zbioru treningowego i walidacyjnego\n",
    "transform_with_aug = torchvision.transforms.Compose([\n",
    "    torchvision.transforms.RandomRotation(15),\n",
    "    torchvision.transforms.RandomHorizontalFlip(),\n",
    "    torchvision.transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2),\n",
    "    torchvision.transforms.ToTensor(),\n",
    "    torchvision.transforms.Normalize(mean=mean, std=std)\n",
    "])\n",
    "\n",
    "# zbiór testowy pozostawiamy bez augmentacji\n",
    "transform_norm = torchvision.transforms.Compose([\n",
    "    torchvision.transforms.ToTensor(),\n",
    "    torchvision.transforms.Normalize(mean=mean, std=std)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82fbd91e",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_ds_with_aug = torchvision.datasets.ImageFolder(root=\"data\", transform=transform_with_aug)\n",
    "train_val_ds_with_aug = torch.utils.data.Subset(full_ds_with_aug, train_val_ds.indices)\n",
    "\n",
    "# zbiór treningowy z augmentacją, walidaycjny i testowy pozostają takie same\n",
    "train_ds_aug, _ = torch.utils.data.random_split(train_val_ds_with_aug, [train_size, val_size])\n",
    "\n",
    "# tworzymy nowy loader\n",
    "train_loader_aug = torch.utils.data.DataLoader(train_ds_aug, batch_size=64, shuffle=True, num_workers=0)\n",
    "\n",
    "print(f\"Nowy zbiór treningowy (z augmentacją): {len(train_ds_aug)} obrazków\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0a75ffb",
   "metadata": {},
   "outputs": [],
   "source": [
    "model2_aug = CNN2(input_size=3, num_classes=len(class_names))\n",
    "optimizer2_aug = torch.optim.Adam(model2_aug.parameters(), lr=0.001)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "epochs = 30\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model2_aug.train()\n",
    "    train_loss = 0.0\n",
    "    for images, labels in tqdm(train_loader_aug):\n",
    "        outputs = model2_aug(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        optimizer2_aug.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer2_aug.step()\n",
    "        train_loss += loss.item()\n",
    "\n",
    "    model2_aug.eval()\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in tqdm(val_loader_norm):\n",
    "            outputs = model2_aug(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            val_loss += loss.item()\n",
    "\n",
    "    print(f\"Epoch [{epoch+1}/{epochs}], Train Loss: {train_loss/len(train_loader_aug):.4f}, Val Loss: {val_loss/len(val_loader_norm):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd8df00b",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_model(model2_aug, test_loader_norm, \"Model 2 (CNN2) z augmentacją\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "962a05a0",
   "metadata": {},
   "source": [
    "Na podstawie przedstawionych wyników można wyciągnąć następujące wnioski:\n",
    "* **Poprawa skuteczności:** Zastosowanie augmentacji danych przyniosło wyraźną poprawę we wszystkich kluczowych metrykach\n",
    "* **Lepsza generalizacja:** Średni ważony F1-score podskoczył, co wskazuje, że model lepiej radzi sobie z klasyfikacją wszystkich kategorii, a nie tylko tych najłatwiejszych\n",
    "* **Naprawa słabych punktów:** Augmentacja szczególnie pomogła w klasach, z którymi model miał największy problem, na przykład skuteczność dla klasy spiral oraz smiley wzrosła\n",
    "\n",
    "Eksperyment pokazuje, że augmentacja danych jest skuteczną techniką dla tego zbioru danych. Zwiększenie różnorodności obrazów treningowych pozwoliło modelowi lepiej generalizować na nowe dane."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fb97d43",
   "metadata": {},
   "source": [
    "### Activation maps (Grad-CAM)\n",
    "\n",
    "Grad-CAM pozwala ona na stworzenie \"mapy ciepła\", która wizualizuje, które obszary na obrazie były dla modelu najważniejsze podczas klasyfikacji. Innymi słowy, możemy zobaczyć, na czym model \"skupia swoją uwagę\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97ffb719",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_grad_cam import GradCAM\n",
    "from pytorch_grad_cam.utils.model_targets import ClassifierOutputTarget\n",
    "from pytorch_grad_cam.utils.image import show_cam_on_image\n",
    "\n",
    "model_to_vis = model3\n",
    "target_layer = [model_to_vis.layer4[-1].conv2]\n",
    "n_img_to_show = 5\n",
    "\n",
    "for param in model_to_vis.parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "cam = GradCAM(model=model_to_vis, target_layers=target_layer)\n",
    "\n",
    "fig, axes = plt.subplots(1, n_img_to_show, figsize=(20, 5))\n",
    "fig.suptitle(\"Activation map Grad-CAM dla losowych obrazów testowych\")\n",
    "\n",
    "resize_transform = torchvision.transforms.Compose([\n",
    "    torchvision.transforms.Resize(256),\n",
    "    torchvision.transforms.CenterCrop(224)\n",
    "])\n",
    "\n",
    "random_indices = np.random.choice(len(test_ds), n_img_to_show, replace=False)\n",
    "\n",
    "for i, idx in enumerate(random_indices):\n",
    "    input_tensor, label_idx = test_ds_pretrained[idx]\n",
    "    original_pil_img, _ = test_ds[idx]\n",
    "\n",
    "    input_tensor_batch = input_tensor.unsqueeze(0)\n",
    "    targets = [ClassifierOutputTarget(label_idx)]\n",
    "\n",
    "    grayscale_cam = cam(input_tensor=input_tensor_batch, targets=targets)\n",
    "    grayscale_cam = grayscale_cam[0, :]\n",
    "\n",
    "    resized_pil_img = resize_transform(original_pil_img)\n",
    "    rgb_img_float = np.array(resized_pil_img) / 255.0\n",
    "\n",
    "    visualization = show_cam_on_image(rgb_img_float, grayscale_cam, use_rgb=True)\n",
    "\n",
    "    ax = axes[i]\n",
    "    ax.imshow(visualization)\n",
    "    ax.set_title(f\"Prawdziwa klasa: {class_names[label_idx]}\")\n",
    "    ax.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97b1beb7",
   "metadata": {},
   "source": [
    "Na podstawie wygenerowanych map aktywacji można wyciągnąć następujące wnioski:\n",
    "\n",
    "* **Model nauczył się poprawnych cech:** Wizualizacje pokazują, że model w większości przypadków skupia swoją uwagę na charakterystycznych i istotnych częściach rysunku, a nie na tle czy przypadkowych elementach, na przykład dla klasy smiley aktywacja koncentruje się na oczach i uśmiechu\n",
    "* **Potwierdzenie wysokiej skuteczności:** Taka interpretowalność daje nam pewność, że wysoka dokładność modelu nie jest dziełem przypadku, model nie nauczył się jakichś nieistotnych korelacji (np. koloru tła), ale faktycznie rozpoznaje kluczowe cechy obiektów na rysunkach\n",
    "* **Potencjalne słabości:** W niektórych przypadkach (np. speech_bubble czy anchor) aktywacje są mniej precyzyjne i rozproszone. Może to sugerować, że cechy tych obiektów są mniej jednoznaczne, ale mimo to model był w stanie dokonać poprawnej klasyfikacji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d1d935e",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model3.state_dict(), 'resnet18.pth')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
